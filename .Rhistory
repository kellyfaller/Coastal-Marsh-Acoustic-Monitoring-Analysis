knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(bit64)
library(magrittr)
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')
df<-list.files(path='C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')%>%
lapply(read_csv) %>%
bind_rows
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics')
write.csv(df,"Recordings.csv", row.names = FALSE)
recording_data1<- read.csv("Recordings.csv", header=TRUE, sep = ",", stringsAsFactors=FALSE)
View(recording_data1)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(bit64)
library(magrittr)
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')
df<-list.files(path='C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')%>%
lapply(read_csv) %>%
bind_rows
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics')
write.csv(df,"Recordings.csv", row.names = FALSE)
# Load the metadata dataset (this should be an output of Arbimon backup. Double check that the name of the CSV is the one that it created for you)
metadata <- read.csv("C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/sites.0001.csv")
# merge the metadata into the recording CSV so that you have the site name, not just site number (which is arbitrarily made up by Arbimon)
recording_data<- merge(df, metadata, by = "site_id")
View(recording_data)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(bit64)
library(magrittr)
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')
# bind all CSV files
df<-list.files(path='C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')%>%
lapply(read_csv) %>%
bind_rows
# Load the metadata dataset (this should be an output of Arbimon backup. Double check that the name of the CSV is the one that it created for you)
metadata <- read.csv("C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/sites.0001.csv")
# merge the metadata into the recording CSV so that you have the site name, not just site number (which is arbitrarily made up by Arbimon)
recording_data<- merge(df, metadata, by = "site_id")
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics')
write.csv(recording_data,"Recordings.csv", row.names = FALSE)
# Convert the datetime column to POSIXct format (adjust the format to match your data)
recording_data$datetime <- as.POSIXct(merged_data$datetime, format = "%m/%d/%Y %H:%M:%S")
# Convert the datetime column to POSIXct format (adjust the format to match your data)
recording_data$datetime <- as.POSIXct(recording_data$datetime, format = "%m/%d/%Y %H:%M:%S")
# Convert the datetime column to POSIXct format (adjust the format to match your data)
recording_data$datetime <- as.POSIXct(recording_data$datetime, format = "%m/%d/%Y %H:%M:%S")
# Add a new column with Julian day using lubridate's yday() function
recording_data$Julian_Day <- yday(recording_data$datetime)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(bit64)
library(magrittr)
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')
# bind all CSV files
df<-list.files(path='C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')%>%
lapply(read_csv) %>%
bind_rows
# Load the metadata dataset (this should be an output of Arbimon backup. Double check that the name of the CSV is the one that it created for you)
metadata <- read.csv("C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/sites.0001.csv")
# merge the metadata into the recording CSV so that you have the site name, not just site number (which is arbitrarily made up by Arbimon)
recording_data<- merge(df, metadata, by = "site_id")
# Convert the datetime column to POSIXct format (adjust the format to match your data)
recording_data$datetime <- as.POSIXct(recording_data$datetime, format = "%m/%d/%Y %H:%M:%S")
# Add a new column with Julian day using lubridate's yday() function
recording_data$Julian_Day <- yday(recording_data$datetime)
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics')
write.csv(recording_data,"Recordings.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(bit64)
library(magrittr)
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')
# bind all CSV files
df<-list.files(path='C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')%>%
lapply(read_csv) %>%
bind_rows
# Load the metadata dataset (this should be an output of Arbimon backup. Double check that the name of the CSV is the one that it created for you)
metadata <- read.csv("C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/sites.0001.csv")
# merge the metadata into the recording CSV so that you have the site name, not just site number (which is arbitrarily made up by Arbimon)
recording_data<- merge(df, metadata, by = "site_id")
# Convert the datetime column to POSIXct format (adjust the format to match your data)
recording_data$datetime <- as.POSIXct(recording_data$datetime, format = "%m/%d/%Y %H:%M:%S")
# Add a new column with Julian day using lubridate's yday() function so we can make recording schedule graphics
recording_data$Julian_Day <- yday(recording_data$datetime)
# Add a new column with the year using lubridate's year() function so that we can filter by year
recording_data$Year <- year(recording_data$datetime)
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics')
write.csv(recording_data,"Recordings.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(bit64)
library(magrittr)
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')
# bind all CSV files
df<-list.files(path='C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/Recording Files')%>%
lapply(read_csv) %>%
bind_rows
# Load the metadata dataset (this should be an output of Arbimon backup. Double check that the name of the CSV is the one that it created for you)
metadata <- read.csv("C:/Users/kfaller/Documents/GitHub/Acoustics/Arbimon Data/sites.0001.csv")
# merge the metadata into the recording CSV so that you have the site name, not just site number (which is arbitrarily made up by Arbimon)
recording_data<- merge(df, metadata, by = "site_id")
# Convert the datetime column to POSIXct format (adjust the format to match your data)
recording_data$datetime <- as.POSIXct(recording_data$datetime, format = "%m/%d/%Y %H:%M:%S")
# Add a new column with Julian day using lubridate's yday() function so we can make recording schedule graphics
recording_data$Julian_Day <- yday(recording_data$datetime)
# Add a new column with the year using lubridate's year() function so that we can filter by year
recording_data$Year <- year(recording_data$datetime)
setwd('C:/Users/kfaller/Documents/GitHub/Acoustics')
write.csv(recording_data,"Recordings.csv", row.names = FALSE)
